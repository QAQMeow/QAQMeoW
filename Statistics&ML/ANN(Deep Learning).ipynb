{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络(深度学习)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 3>\n",
    "\n",
    "**人工神经网络(Artificial Neural Network，ANN)** 是指一系列受生物学和神经科学启发的数学模型这些模型主要是通过对人脑的神经元网络进行抽象，构建人工神经元，并按照一定拓扑结构来建立人工神经元之间的连接，来模拟生物神经网络。在人工智能领域，人工神经网络也常常简称为神经网络（Neural\n",
    "Network，NN）或神经模型（Neural Model）。神经网络最早是作为一种主要的连接主义模型. 20 世纪 80 年代中后期，最流行的一种连接主义模型是分布式并行处理模型，其有3个主要特性：\n",
    "1. 信息表示是分布式的（非局部的）\n",
    "2. 记忆和知识是存储在单元之间的连接上\n",
    "3. 通过逐渐改变单元之间的连接强度来学习新的知识.\n",
    "\n",
    "连接主义的神经网络有着多种多样的网络结构以及学习方法，虽然早期模型强调模型的生物可解释性（Biological Plausibility），但后期更关注于对某种特定\n",
    "认知能力的模拟，比如物体识别、语言理解等。\n",
    "\n",
    "\n",
    "**深度学习** 一般指通过训练多层网络结构对未知数据进行分类或递归。<br/>\n",
    "\n",
    "\n",
    "**有监督学习方法：** <br/>\n",
    "+ 深度前馈网络（Deep Feedforward Neural Network, D-FNN）\n",
    "+ 卷积神经网络（Convolutional Neural Network, CNN）\n",
    "+ 循环神经网络（Recurrent Neural Network, RNN）\n",
    "+ 胶囊网络（CapsuleNet）\n",
    "+ 深度森林（Deep Forest）\n",
    "\n",
    "**无监督学习方法：** \n",
    "+ 深度信念网络（Deep Belief Network, DBN）\n",
    "+ 深度玻尔兹曼机（Deep Boltzmann Machine, DBM）\n",
    "+ 深度自编码器（Deep Auto-Encoder, DAE）\n",
    "+ 降噪自编码器（Denoising Auto-Encoder, D-AE）\n",
    "+ 栈式自编码器（Stacked Auto-Encoder, SAE）\n",
    "+ 生成对抗网络（Generative Adversarial Networks，GAN）\n",
    "+ 非参数贝叶斯网络（Non-parametric Bayesian networks）\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经元模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 3>\n",
    "\n",
    "\n",
    "**人工神经元（Artificial Neuron）** ，简称神经元（Neuron），是构成神经网络的基本单元，其主要是模拟生物神经元的结构和特性，接收一组输入信号并产生输出。\n",
    "\n",
    "<img src= ./Pic/Artificial_Neuron.png width=400/>\n",
    "\n",
    "假设一个神经元接收𝐷 个输入 $𝑥_1,x_2...x_D$ ，令向量 $ x =[𝑥_1,x_2...x_D] $ 来表示这组输入，净输入也叫净活性值（Net Activation）。<br/>\n",
    "并用净输入（Net Input）𝑧 ∈ ℝ表示一个神经元所获得的输入信号𝒙的加权和<br/><br/>\n",
    "                    $z = \\sum_{d=1}^D \\omega_dx_d+b =\\omega^T+b$<br/><br/>\n",
    "其中 $\\omega = [\\omega_1,\\omega_2,⋯ ,\\omega_𝐷]$  ∈ ℝ𝐷 是𝐷 维的权重向量，𝑏 ∈ ℝ是偏置。 <br/>\n",
    "净输入 𝑧 在经过一个非线性函数 𝑓(⋅) 后，得到神经元的活性值（Activation）𝑎，𝑎 = 𝑓(𝑧)。<br/>\n",
    "其中非线性函数𝑓(⋅)称为激活函数（Activation Function）。\n",
    "\n",
    "激活函数在神经元中非常重要的. 为了增强网络的表示能力和学习能力，激活函数需要具备以下几点性质：\n",
    "\n",
    "1. 连续并可导（允许少数点上不可导）的非线性函数. 可导的激活函数可以直接利用数值优化的方法来学习网络参数\n",
    "2. 激活函数及其导函数要尽可能的简单，有利于提高网络计算效率\n",
    "3. 激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性\n",
    "\n",
    "*MP神经元中的激活函数 𝑓 为 0 或 1 的阶跃函数，而现代神经元中的激活函数通常要求是连续可导的函数.*\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常用的激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 3>\n",
    "    \n",
    "### Logistic函数\n",
    "\n",
    "### Tanh函数\n",
    "\n",
    "### Hard-Logistic函数和Hard-Tanh函数\n",
    "\n",
    "### ReLU函数\n",
    "\n",
    "### 带泄露的ReLU\n",
    "\n",
    "### 带参数的ReLU\n",
    "\n",
    "### ELU函数\n",
    "\n",
    "### Softplus函数\n",
    "\n",
    "### Swish函数\n",
    "\n",
    "### 高斯误差线性单元\n",
    "\n",
    "### Maxout单元\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知器 Multi-Layer Perceptron，MLP（前馈神经网络 Feedforward Neural Network，FNN）\n",
    "<font size = 3>\n",
    "<br/>\n",
    "    \n",
    "在前馈神经网络中，各神经元分别属于不同的层. 每一层的神经元可以接收前一层神经元的信号，并产生信号输出到下一层. 第0层称为输入层，最后一层称为输出层，其他中间层称为隐藏层. 整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示。\n",
    "<img src=./Pic/多层前馈神经网络.png width = 500/>\n",
    "<img src=./Pic/Marker.png width = 500/>\n",
    "\n",
    "令$\\boldsymbol{a}^{(0)} =\\boldsymbol{x}$,前馈神经网络通过不断迭代如下两式：\n",
    "\n",
    "$\\boldsymbol{z}^{(l)}= \\boldsymbol{W}^{(l)} \\boldsymbol{a}^{(l-1)}+ \\boldsymbol{b}^{(l)}$<br/>\n",
    "$\\boldsymbol{a}^{(l)}=f_l(\\boldsymbol{z}^{(l)})$\n",
    "\n",
    "上两式可写合并成：$\\boldsymbol{z}^{(𝑙)} = \\boldsymbol{W}^{(𝑙)} ⋅ 𝑓_{𝑙−1}(\\boldsymbol{z}^{(𝑙−1)}) + \\boldsymbol{b}^{(𝑙)}$<br/>\n",
    "首先根据第$𝑙 − 1$层神经元的活性值（Activation）$\\boldsymbol{a}^{(𝑙−1)}$ 计算出第$𝑙$层神经元的净活性值（Net Activation）$\\boldsymbol{z}^{(𝑙)}$，然后经过一个激活函数得到第 $𝑙$ 层神经元的活性值。\n",
    "\n",
    "这样，前馈神经网络可以通过逐层的信息传递，得到网络最后的输出 $\\boldsymbol{a}^{(𝐿)}$。整个网络可以看作一个复合函数$\\phi (\\boldsymbol{x};\\boldsymbol{W},\\boldsymbol{b})$，将向量$\\boldsymbol{x}$作为第1层的输入$\\boldsymbol{a}^{(0)}$，将第𝐿层的输出$\\boldsymbol{a}^{(𝐿)}$ 作为整个函数的输出:\n",
    "\n",
    "$\\boldsymbol{x} = \\boldsymbol{a}^{(0)} \\to \\boldsymbol{z}^{(1)} \\to \\boldsymbol{a}^{(1)} \\to \\boldsymbol{z}^{(2)} \\to ⋯ \\to \\boldsymbol{a}^{(L-1)} \\to\\boldsymbol{z}^{(L)} \\to \\boldsymbol{a}^{(𝐿)} = \\phi (\\boldsymbol{x};\\boldsymbol{W},\\boldsymbol{b})$<br/>\n",
    "其中$\\boldsymbol{W},\\boldsymbol{b}$表示网络中所有层的连接权重和偏置\n",
    "\n",
    "根据通用近似定理，对于具有线性输出层和至少一个使用“挤压”性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够，它可以以任意的精度来近似任何一个定义在实数空间 $R^D$ 中的有界闭集函数 所谓“挤压”性质的函数是指像Sigmoid函数的有界函数，但神经网络的通用近似性质也被证明对于其他类型的激活函数，比如ReLU，也都是适用的。通用近似定理只是说明了神经网络的计算能力可以去近似一个给定的连续函数，但并没有给出如何找到这样一个网络，以及是否是最优的。根据通用近似定理，神经网络在某种程度上可以作为一个“万能”函数来使用，可以用来进行复杂的特征转换，或逼近一个复杂的条件分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播算法\n",
    "<br/>\n",
    "<font size=3>\n",
    "\n",
    "链式法则\n",
    "\n",
    "神经网络的参数主要通过梯度下降来进行优化. 当确定了风险函数以及网络结构后，我们就可以手动用链式法则来计算风险函数对每个参数的梯度，并用代码进行实现."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化问题\n",
    "<br/>\n",
    "<font size=3>\n",
    "    \n",
    "### 非凸优化问题\n",
    "\n",
    "### 梯度消失问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
